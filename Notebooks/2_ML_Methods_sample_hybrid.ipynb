{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1fa080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56876763",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ccd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 변수 수 : 60\n",
      "제거된 변수: ['Z_KDBDRS', 'K_ODD', 'KDBDRS', 'wt_s', 'DICCD_CP', 'ODD_Cur', 'ODD_Pas', 'DICCD_C', 'DICCD_P', 'MentD', 'ZAI_Incom', 'Z_K_ODD', 'Z_K_CD', 'Z_K_IA', 'Z_K_HI', 'Z_GAD', 'Z_PHQ', 'Z_SAS']\n",
      "남은 변수 수: 43\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"/workspace/data/0_Subtask/[DICCD 분석] 251107.csv\" # ~251110\n",
    "data_path = \"/workspace/data/0_Subtask/[DICCD 분석] 251111_SRD 추가.csv\" # 251111\n",
    "raw_df = pd.read_csv(data_path)\n",
    "\n",
    "# y = 타겟 변수 / x = 예측 변수\n",
    "# except_cols = ['DICCD_CP', 'ODD_Cur', 'ODD_Pas', 'DICCD_C', 'DICCD_P', 'MentD']\n",
    "except_cols = ['Z_KDBDRS', 'K_ODD', 'KDBDRS', 'wt_s', 'DICCD_CP', 'ODD_Cur', 'ODD_Pas', 'DICCD_C', 'DICCD_P', 'MentD']\n",
    "standard_cols = ['ZAI_Incom', 'Z_K_ODD', 'Z_K_CD', 'Z_K_IA', 'Z_K_HI', 'Z_GAD', 'Z_PHQ', 'Z_SAS']\n",
    "nonstandard_cols = ['Incom', 'K_ODD', 'K_CD', 'K_IA', 'K_HI', 'GAD', 'PHQ', 'SAS']\n",
    "\n",
    "\n",
    "drop_cols = except_cols + nonstandard_cols\n",
    "# drop_cols = except_cols + standard_cols\n",
    "\n",
    "print(f\"기존 변수 수 : {len(raw_df.columns)}\")\n",
    "raw_df = raw_df.drop(columns=drop_cols) # 입력 변수 \n",
    "print(f\"제거된 변수: {except_cols + standard_cols}\")\n",
    "print(f\"남은 변수 수: {len(raw_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e00fba",
   "metadata": {},
   "source": [
    "## Missing value imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "904094df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 칼럼들 : ['GEdu', 'GAlc', 'GTob']\n",
      "\n",
      "==== 결측치가 25% 미만인 컬럼만 남김 ====\n",
      "남은 변수 수: 40\n",
      "남은 컬럼 리스트: ['Sex', 'Answ', 'Z_K_IA', 'Z_K_HI', 'Z_K_ODD', 'Z_K_CD', 'ODD_CP', 'Age_Grp', 'MEdu', 'FEdu', 'P_Marr', 'MJob', 'FJob', 'ZAI_Incom', 'MAlc', 'FAlc', 'MTob', 'FTob', 'PSleep', 'Z_GAD', 'Z_PHQ', 'PAF', 'SBV', 'SBP', 'CBV', 'CBP', 'Avg_G', 'GDec', 'BF', 'RFG', 'AdolSlp', 'MoodD', 'AnxD', 'ST1', 'ST2', 'ST3', 'ST4', 'IGD_P', 'Z_SAS', 'SRD_CP']\n",
      "\n",
      "==== 남아있는 컬럼 중 결측치가 있는 컬럼의 결측치 개수 ====\n",
      "MEdu      56\n",
      "FEdu     207\n",
      "MJob      56\n",
      "FJob     207\n",
      "MAlc      56\n",
      "FAlc     207\n",
      "MTob      56\n",
      "FTob     207\n",
      "IGD_P    743\n",
      "dtype: int64\n",
      "\n",
      "결측치가 남아있는 변수 수: 9 / 전체 변수 수: 40\n"
     ]
    }
   ],
   "source": [
    "from utils.data_imputation import filter_by_missing_ratio\n",
    "# 사용 예시\n",
    "df = filter_by_missing_ratio(raw_df, threshold=0.25, visualize = False)\n",
    "X = df.drop(columns=['ODD_CP'])\n",
    "y = df['ODD_CP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e09bd94",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess_check",
   "metadata": {},
   "source": [
    "### 데이터 전처리 검토\n",
    "\n",
    "1. 범주형\n",
    "\n",
    "    1-1 Ordinal : Imputation(Median) => OrdinalEncoder\n",
    "\n",
    "    1-2 Nominal : Imputation(Unknown) => OneHotEncoder\n",
    "\n",
    "2. 수치형\n",
    "\n",
    "    2-1 결측치 존재 시, Imputation(Median) 적용 => Z-표준화(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "preprocess_check_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ integer 전처리 중...\n",
      "▶ float 전처리 중...\n",
      "▶ 순서 있는 category(ordinal) 전처리 중...\n",
      "▶ object 전처리 중...\n",
      "▶ 순서 없는 category 전처리 중...\n",
      "✅ 데이터 전처리 완료\n"
     ]
    }
   ],
   "source": [
    "from utils.data_preprocessor import check_preprocessing_needs, preprocess_dataframe\n",
    "from utils.data_preprocessor import data_preprocess_pipeline\n",
    "from utils.data_analyzer import analyze_correlation_matrix\n",
    "# 전처리 필요사항 검토\n",
    "# recommendations = check_preprocessing_needs(X_train, target_col='ODD_CP')\n",
    "\n",
    "# # 권장사항에 따라 전처리 (선택사항)\n",
    "X = preprocess_dataframe(\n",
    "    X, \n",
    "    target_col='ODD_CP',\n",
    "    drop_weight=True,  # 가중치 변수 제거\n",
    "    convert_categorical=['Answ', 'IGD_P', 'FEdu', 'MEdu', 'FJob', 'MJob', 'Age_Grp', 'P_Marr'],\n",
    "    convert_ordinal=['ST1', 'ST2', 'ST3', 'ST4', 'PAF', 'MAlc', 'FAlc', \"MTob\", \"FTob\", \"MAlc\", \"FAlc\", \"GAlc\", \"MTob\", \"FTob\", \"GTob\"], \n",
    "    convert_binary=['SRD_CP', 'IGD_P', 'Sex', 'PSleep', 'SBV', 'SBP', 'CBV', 'CBP', 'GDec', 'BF', 'RFG', 'MentD', 'AdolSlp', 'MoodD', 'AnxD'],\n",
    "    drop_low_variance=False,  # 분산이 낮은 변수 제거\n",
    "    drop_leakage=True  # 데이터 누수 위험 변수 제거\n",
    ")\n",
    "\n",
    "X = data_preprocess_pipeline(X) # 전처리된 데이터 \n",
    "\n",
    "# X_preprocessed.info()\n",
    "# analyze_correlation_matrix(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f354e",
   "metadata": {},
   "source": [
    "### Startified Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dae45744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "데이터 분할 및 샘플링\n",
      "============================================================\n",
      "\n",
      "전체 데이터:\n",
      "  클래스 0: 3205개\n",
      "  클래스 1: 177개\n",
      "\n",
      "1단계: Test set 구성 (각 클래스 60개씩)\n",
      "  Test set: 클래스 0=60개, 클래스 1=60개 (총 120개)\n",
      "\n",
      "2단계: Train set용 원본 데이터 (Test set 제외)\n",
      "  Train 원본: 클래스 0=3145개, 클래스 1=117개\n",
      "\n",
      "3단계: Train set 샘플링\n",
      "  클래스 0 언더샘플링: 3145개 → 240개\n",
      "  클래스 1 오버샘플링: 117개 → 147개 (SMOTEENN 사용)\n",
      "\n",
      "4단계: Train set 최종 구성\n",
      "  Train set: 클래스 0=240개, 클래스 1=147개 (총 387개)\n",
      "  Train 비율: 1:1.6\n",
      "\n",
      "최종 데이터 분포:\n",
      "  Train: 387개 (클래스 0=240, 클래스 1=147)\n",
      "  Test:  120개 (클래스 0=60, 클래스 1=60)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from utils.data_splitter import oversample_train_test_split\n",
    "# 함수 사용\n",
    "df = pd.concat([X, y], axis=1)\n",
    "X_train, X_test, y_train, y_test = oversample_train_test_split(\n",
    "    X, y, \n",
    "    target_col='ODD_CP',\n",
    "    test_size_per_class=60,\n",
    "    train_size_per_class=240,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    method = \"SMOTEENN\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541f37e",
   "metadata": {},
   "source": [
    "# Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Fold 1 Validation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.979861</td>\n",
       "      <td>0.40201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.923077   0.852941  0.966667   0.90625       0.979861         0.40201"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.906944</td>\n",
       "      <td>0.40201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.816667   0.895833  0.716667  0.796296       0.906944         0.40201"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 2 =====\n",
      "Fold 2 Validation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.95082</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.552764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.961538   0.935484  0.966667   0.95082       0.989583        0.552764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.915833</td>\n",
       "      <td>0.552764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0     0.825   0.882353    0.75  0.810811       0.915833        0.552764"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 3 =====\n",
      "Fold 3 Validation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.974026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.99569</td>\n",
       "      <td>0.919598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.974026        1.0  0.931034  0.964286        0.99569        0.919598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.919598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0      0.75   0.916667    0.55    0.6875       0.921667        0.919598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 4 =====\n",
      "Fold 4 Validation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.90566</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.974874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.935065        1.0  0.827586   0.90566       0.979167        0.974874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.974874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.666667   0.884615  0.383333  0.534884       0.911111        0.974874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 5 =====\n",
      "Fold 5 Validation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935065</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.976293</td>\n",
       "      <td>0.487437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.935065        0.9  0.931034  0.915254       0.976293        0.487437"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "      <th>Best_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.920278</td>\n",
       "      <td>0.487437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  ROC AUC Score  Best_Threshold\n",
       "0  0.816667   0.865385    0.75  0.803571       0.920278        0.487437"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Val F1 Score 기준으로 Softmax 변환한 가중치 =====\n",
      "===== Stratified K-Fold Validation 평균 성능 (F1 Score Weighted) =====\n",
      "\n",
      "== CatBoost Model (Validation) ==\n",
      "Fold weights (by F1 Score): [0.0023690490020396634, 0.20427269119496738, 0.7852954671414462, 0.002233402808448755, 0.005829389853098059]\n",
      "Accuracy: 0.971040\n",
      "Precision: 0.985890\n",
      "Recall: 0.938167\n",
      "F1 Score: 0.960981\n",
      "ROC AUC Score: 0.994255\n",
      "Best Thresholds per fold: [0.402  0.5528 0.9196 0.9749 0.4874]  (mean: 0.6673)\n",
      "\n",
      "===== Stratified K-Fold Test 평균 성능 (F1 Score Weighted) =====\n",
      "== CatBoost Model (Test Set) ==\n",
      "Accuracy: 0.765681\n",
      "Precision: 0.909237\n",
      "Recall: 0.592043\n",
      "F1 Score: 0.713283\n",
      "ROC AUC Score: 0.920409\n",
      "\n",
      "--- 각 Fold 별 Validation 결과 ---\n",
      "Fold 1:\n",
      "  Accuracy: 0.923077\n",
      "  Precision: 0.852941\n",
      "  Recall: 0.966667\n",
      "  F1 Score: 0.906250\n",
      "  ROC AUC Score: 0.979861\n",
      "  Best_Threshold: 0.402010\n",
      "Fold 2:\n",
      "  Accuracy: 0.961538\n",
      "  Precision: 0.935484\n",
      "  Recall: 0.966667\n",
      "  F1 Score: 0.950820\n",
      "  ROC AUC Score: 0.989583\n",
      "  Best_Threshold: 0.552764\n",
      "Fold 3:\n",
      "  Accuracy: 0.974026\n",
      "  Precision: 1.000000\n",
      "  Recall: 0.931034\n",
      "  F1 Score: 0.964286\n",
      "  ROC AUC Score: 0.995690\n",
      "  Best_Threshold: 0.919598\n",
      "Fold 4:\n",
      "  Accuracy: 0.935065\n",
      "  Precision: 1.000000\n",
      "  Recall: 0.827586\n",
      "  F1 Score: 0.905660\n",
      "  ROC AUC Score: 0.979167\n",
      "  Best_Threshold: 0.974874\n",
      "Fold 5:\n",
      "  Accuracy: 0.935065\n",
      "  Precision: 0.900000\n",
      "  Recall: 0.931034\n",
      "  F1 Score: 0.915254\n",
      "  ROC AUC Score: 0.976293\n",
      "  Best_Threshold: 0.487437\n",
      "\n",
      "--- 각 Fold 별 Test 결과 ---\n",
      "Fold 1:\n",
      "  Accuracy: 0.816667\n",
      "  Precision: 0.895833\n",
      "  Recall: 0.716667\n",
      "  F1 Score: 0.796296\n",
      "  ROC AUC Score: 0.906944\n",
      "  Best_Threshold: 0.402010\n",
      "Fold 2:\n",
      "  Accuracy: 0.825000\n",
      "  Precision: 0.882353\n",
      "  Recall: 0.750000\n",
      "  F1 Score: 0.810811\n",
      "  ROC AUC Score: 0.915833\n",
      "  Best_Threshold: 0.552764\n",
      "Fold 3:\n",
      "  Accuracy: 0.750000\n",
      "  Precision: 0.916667\n",
      "  Recall: 0.550000\n",
      "  F1 Score: 0.687500\n",
      "  ROC AUC Score: 0.921667\n",
      "  Best_Threshold: 0.919598\n",
      "Fold 4:\n",
      "  Accuracy: 0.666667\n",
      "  Precision: 0.884615\n",
      "  Recall: 0.383333\n",
      "  F1 Score: 0.534884\n",
      "  ROC AUC Score: 0.911111\n",
      "  Best_Threshold: 0.974874\n",
      "Fold 5:\n",
      "  Accuracy: 0.816667\n",
      "  Precision: 0.865385\n",
      "  Recall: 0.750000\n",
      "  F1 Score: 0.803571\n",
      "  ROC AUC Score: 0.920278\n",
      "  Best_Threshold: 0.487437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CatBoostFoldTrainer at 0x7f71225ba390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "class CatBoostFoldTrainer:\n",
    "    def __init__(self, model_params=None, n_splits=5, random_state=123, verbose=100, T=0.01):\n",
    "        if model_params is None:\n",
    "            model_params = dict(\n",
    "                iterations=1000, learning_rate=0.38577, depth=8, l2_leaf_reg=9.587765, subsample=0.748324, random_strength=0.0, class_weights = [1, 10],\n",
    "                min_data_in_leaf=59, leaf_estimation_iterations=1, loss_function='Logloss', eval_metric='AUC', verbose=verbose, random_seed=random_state\n",
    "            )\n",
    "        self.model_params = dict(model_params)\n",
    "        self.model_params['verbose'] = False\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.T = T\n",
    "\n",
    "        self.metrics = { 'CatBoost': [] }  # validation metrics (per fold)\n",
    "        self.test_metrics = { 'CatBoost': [] }  # NEW: test set metrics (per fold)\n",
    "        self.feature_importances = { 'CatBoost': [] }\n",
    "        self.test_proba = { 'CatBoost': [] }\n",
    "        self.test_preds = { 'CatBoost': [] }\n",
    "        self.fold_thresholds = { 'CatBoost': [] }\n",
    "        self.shap_values_train = { 'CatBoost': [] }  # SHAP values for train set (per fold)\n",
    "        self.shap_values_test = { 'CatBoost': [] }  # SHAP values for test set (per fold)\n",
    "        self.fold_weights = None\n",
    "        self.weighted_avg_metrics = None\n",
    "        self.weighted_avg_test_metrics = None  # NEW: weighted test set metrics\n",
    "\n",
    "    def fit(self, X, y, X_test, y_test=None):\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        self.metrics['CatBoost'].clear()\n",
    "        self.test_metrics['CatBoost'].clear()\n",
    "        self.feature_importances['CatBoost'].clear()\n",
    "        self.test_proba['CatBoost'].clear()\n",
    "        self.test_preds['CatBoost'].clear()\n",
    "        self.fold_thresholds['CatBoost'].clear()\n",
    "        self.shap_values_train['CatBoost'].clear()\n",
    "        self.shap_values_test['CatBoost'].clear()\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "            print(f\"===== Fold {fold} =====\")\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            cat_model = CatBoostClassifier(**self.model_params)\n",
    "            cat_model.fit(X_train, y_train)\n",
    "            \n",
    "            val_proba = cat_model.predict_proba(X_val)[:, 1]\n",
    "            thresholds = np.linspace(0, 1, 200)\n",
    "            f1s = [f1_score(y_val, (val_proba >= t).astype(int)) for t in thresholds]\n",
    "            best_idx = np.argmax(f1s)\n",
    "            best_threshold = thresholds[best_idx]\n",
    "            val_pred_best = (val_proba >= best_threshold).astype(int)\n",
    "            val_metrics = {\n",
    "                'Accuracy': accuracy_score(y_val, val_pred_best),\n",
    "                'Precision': precision_score(y_val, val_pred_best),\n",
    "                'Recall': recall_score(y_val, val_pred_best),\n",
    "                'F1 Score': f1_score(y_val, val_pred_best),\n",
    "                'ROC AUC Score': roc_auc_score(y_val, val_proba),\n",
    "                'Best_Threshold': best_threshold\n",
    "            }\n",
    "            self.metrics['CatBoost'].append(val_metrics)\n",
    "            self.feature_importances['CatBoost'].append(cat_model.feature_importances_)\n",
    "            \n",
    "            # Calculate SHAP values for train and test sets\n",
    "            explainer = shap.TreeExplainer(cat_model)\n",
    "            shap_values_train_fold = explainer.shap_values(X_train)\n",
    "            shap_values_test_fold = explainer.shap_values(X_test)\n",
    "            # For binary classification, shap_values is a list [values_for_class_0, values_for_class_1]\n",
    "            # We take values_for_class_1 (positive class)\n",
    "            if isinstance(shap_values_train_fold, list):\n",
    "                shap_values_train_fold = shap_values_train_fold[1]\n",
    "            if isinstance(shap_values_test_fold, list):\n",
    "                shap_values_test_fold = shap_values_test_fold[1]\n",
    "            self.shap_values_train['CatBoost'].append(shap_values_train_fold)\n",
    "            self.shap_values_test['CatBoost'].append(shap_values_test_fold)\n",
    "            \n",
    "            test_proba = cat_model.predict_proba(X_test)[:, 1]\n",
    "            test_pred = (test_proba >= best_threshold).astype(int)\n",
    "            self.test_proba['CatBoost'].append(test_proba)\n",
    "            self.test_preds['CatBoost'].append(test_pred)\n",
    "            self.fold_thresholds['CatBoost'].append(best_threshold)\n",
    "\n",
    "            # Compute and store test set metrics for each fold if y_test is provided\n",
    "            if y_test is not None:\n",
    "                try:\n",
    "                    test_metric_this_fold = {\n",
    "                        'Accuracy': accuracy_score(y_test, test_pred),\n",
    "                        'Precision': precision_score(y_test, test_pred),\n",
    "                        'Recall': recall_score(y_test, test_pred),\n",
    "                        'F1 Score': f1_score(y_test, test_pred),\n",
    "                        'ROC AUC Score': roc_auc_score(y_test, test_proba),\n",
    "                        'Best_Threshold': best_threshold\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    # In case of error, e.g. only one class in y_test, fill with np.nan\n",
    "                    test_metric_this_fold = {\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Precision': np.nan,\n",
    "                        'Recall': np.nan,\n",
    "                        'F1 Score': np.nan,\n",
    "                        'ROC AUC Score': np.nan,\n",
    "                        'Best_Threshold': best_threshold\n",
    "                    }\n",
    "                self.test_metrics['CatBoost'].append(test_metric_this_fold)\n",
    "            \n",
    "            print(f\"Fold {fold} Validation Metrics\")\n",
    "            display(pd.DataFrame([val_metrics]))\n",
    "            if y_test is not None:\n",
    "                print(f\"Fold {fold} Test Metrics\")\n",
    "                display(pd.DataFrame([test_metric_this_fold]))\n",
    "\n",
    "        self.y_test = y_test if y_test is not None else None\n",
    "        self.print_fold_results(y_test = y_test)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_with_threshold(y_true, proba, threshold):\n",
    "        y_pred = (proba >= threshold).astype(int)\n",
    "        return {\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred),\n",
    "            'Recall': recall_score(y_true, y_pred),\n",
    "            'F1 Score': f1_score(y_true, y_pred),\n",
    "            'ROC AUC Score': roc_auc_score(y_true, proba),\n",
    "        }\n",
    "\n",
    "    def calc_softmax_weights(self):\n",
    "        f1_scores = np.array([fold_metric['F1 Score'] for fold_metric in self.metrics['CatBoost']])\n",
    "        exp_scores = np.exp(f1_scores / self.T)\n",
    "        total_exp = np.sum(exp_scores)\n",
    "        self.fold_weights = exp_scores / total_exp\n",
    "        return self.fold_weights\n",
    "\n",
    "    def calculate_weighted_metrics(self):\n",
    "        fold_weights = self.calc_softmax_weights()\n",
    "        model_metrics = self.metrics['CatBoost']\n",
    "        metric_keys = [k for k in model_metrics[0] if k != 'Best_Threshold']\n",
    "        self.weighted_avg_metrics = {\n",
    "            metric: sum(fw * fold_metric[metric] for fw, fold_metric in zip(fold_weights, model_metrics))\n",
    "            for metric in metric_keys\n",
    "        }\n",
    "        return self.weighted_avg_metrics\n",
    "\n",
    "    def calculate_weighted_test_metrics(self):\n",
    "        # Weighted avg of test metrics using F1-score-based fold weights\n",
    "        if len(self.test_metrics['CatBoost']) == 0:\n",
    "            return None\n",
    "        fold_weights = self.calc_softmax_weights()\n",
    "        model_metrics = self.test_metrics['CatBoost']\n",
    "        metric_keys = [k for k in model_metrics[0] if k != 'Best_Threshold']\n",
    "        self.weighted_avg_test_metrics = {\n",
    "            metric: sum(fw * fold_metric[metric] for fw, fold_metric in zip(fold_weights, model_metrics))\n",
    "            for metric in metric_keys\n",
    "        }\n",
    "        return self.weighted_avg_test_metrics\n",
    "\n",
    "    def print_fold_results(self, y_test=None, mode_type=None):\n",
    "        # y_test를 인자로 넘겨받지 않으면 self.y_test를 사용\n",
    "        if y_test is None and hasattr(self, 'y_test'):\n",
    "            y_test = self.y_test\n",
    "\n",
    "        print(\"===== Val F1 Score 기준으로 Softmax 변환한 가중치 =====\")\n",
    "        fold_weights = self.calc_softmax_weights()\n",
    "        avg_metrics = self.calculate_weighted_metrics()\n",
    "        print(\"===== Stratified K-Fold Validation 평균 성능 (F1 Score Weighted) =====\")\n",
    "        print(f\"\\n== CatBoost Model (Validation) ==\")\n",
    "        print(\"Fold weights (by F1 Score):\", fold_weights.tolist())\n",
    "        for metric, value in avg_metrics.items():\n",
    "            print(f\"{metric}: {value:.6f}\")\n",
    "        avg_thr = np.mean(self.fold_thresholds[\"CatBoost\"])\n",
    "        print(f\"Best Thresholds per fold: {np.round(self.fold_thresholds['CatBoost'],4)}  (mean: {avg_thr:.4f})\")\n",
    "        \n",
    "        if y_test is not None and len(self.test_metrics['CatBoost']) > 0:\n",
    "            avg_test_metrics = self.calculate_weighted_test_metrics()\n",
    "            print(\"\\n===== Stratified K-Fold Test 평균 성능 (F1 Score Weighted) =====\")\n",
    "            print(f\"== CatBoost Model (Test Set) ==\")\n",
    "            for metric, value in avg_test_metrics.items():\n",
    "                print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "        if mode_type is None:\n",
    "            print(\"\\n--- 각 Fold 별 Validation 결과 ---\")\n",
    "            for i, metrics in enumerate(self.metrics[\"CatBoost\"]):\n",
    "                print(f\"Fold {i+1}:\")\n",
    "                for metric, value in metrics.items():\n",
    "                    print(f\"  {metric}: {value:.6f}\")\n",
    "            if y_test is not None and len(self.test_metrics['CatBoost']) > 0:\n",
    "                print(\"\\n--- 각 Fold 별 Test 결과 ---\")\n",
    "                for i, metrics in enumerate(self.test_metrics[\"CatBoost\"]):\n",
    "                    print(f\"Fold {i+1}:\")\n",
    "                    for metric, value in metrics.items():\n",
    "                        print(f\"  {metric}: {value:.6f}\")\n",
    "        elif mode_type == \"soft_voting\":\n",
    "            if y_test is not None and 'CatBoost' in self.test_proba:\n",
    "                test_probas = np.array(self.test_proba['CatBoost'])  # (n_folds, n_samples)\n",
    "                test_preds = np.array(self.test_preds['CatBoost'])   # (n_folds, n_samples)\n",
    "\n",
    "                # Hard voting\n",
    "                hard_voting_pred = mode(test_preds, axis=0, keepdims=False).mode\n",
    "\n",
    "                print(f\":: [Test Set, Hard Voting (Majority)] ::\")\n",
    "                print(f\"Test Accuracy: {accuracy_score(y_test, hard_voting_pred):.6f}\")\n",
    "                print(f\"Test Precision: {precision_score(y_test, hard_voting_pred):.6f}\")\n",
    "                print(f\"Test Recall: {recall_score(y_test, hard_voting_pred):.6f}\")\n",
    "                print(f\"Test F1 Score: {f1_score(y_test, hard_voting_pred):.6f}\")\n",
    "\n",
    "    def get_val_metrics(self):\n",
    "        return self.metrics\n",
    "\n",
    "    def get_test_metrics(self):\n",
    "        return self.test_metrics\n",
    "\n",
    "    def get_feature_importances(self):\n",
    "        return self.feature_importances\n",
    "\n",
    "    def get_test_labels(self):\n",
    "        return self.y_test\n",
    "\n",
    "    def get_test_proba(self):\n",
    "        return self.test_proba\n",
    "\n",
    "    def get_test_preds(self):\n",
    "        return self.test_preds\n",
    "\n",
    "    def get_fold_thresholds(self):\n",
    "        return self.fold_thresholds\n",
    "\n",
    "    def get_shap_values_train(self):\n",
    "        return self.shap_values_train\n",
    "\n",
    "    def get_shap_values_test(self):\n",
    "        return self.shap_values_test\n",
    "\n",
    "# 사용 예시:\n",
    "catboost_cv = CatBoostFoldTrainer(n_splits=5, random_state=42, T=0.01)\n",
    "catboost_cv.fit(X_train, y_train, X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee338e",
   "metadata": {},
   "source": [
    "## Softmax 변환 후 평균 성능 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft voting (평균 확률, best F1 기준 threshold 사용)\n",
    "feature_importances = catboost_cv.get_feature_importances()\n",
    "save_dict = {\n",
    "    'test_inputs': X_test,\n",
    "    'test_labels': catboost_cv.get_test_labels(),\n",
    "    'test_proba': catboost_cv.get_test_proba(),\n",
    "    'test_preds': catboost_cv.get_test_preds(),\n",
    "    'test_metrics': catboost_cv.get_test_metrics(),\n",
    "    'fold_thresholds': catboost_cv.get_fold_thresholds(),\n",
    "    'shap_values_test': catboost_cv.get_shap_values_test(),\n",
    "    'feature_importances': pd.DataFrame({'feature': X_train.columns, 'importance': np.mean(feature_importances['CatBoost'], axis=0)}).sort_values(by='importance', ascending=False),\n",
    "}\n",
    "\n",
    "import pickle\n",
    "save_path = \"/workspace/data/results/catboost_downsample_results.pkl\"\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a6c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8578112f",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d5752",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0758fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CatBoost feature importance의 각 fold별 평균을 계산하여 테이블로 생성 및 정렬, 소수점 3자리까지 표시\n",
    "# catboost_importances = np.mean(feature_importances['CatBoost'], axis=0)\n",
    "# feature_importance_df = pd.DataFrame({\n",
    "#     'feature': X_train.columns,\n",
    "#     'importance': catboost_importances\n",
    "# }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "# feature_importance_df['importance'] = feature_importance_df['importance'].round(3)\n",
    "# display(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42471f29",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f928b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from catboost import CatBoostClassifier\n",
    "# from utils.ml_pipeline import evaluate_model\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# import optuna\n",
    "# import numpy as np\n",
    "# import warnings\n",
    "# import os\n",
    "# os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "# warnings.filterwarnings('ignore')\n",
    "# # CatBoost GPU에서 \"Default metric period is 5 because AUC is/are not implemented for GPU\"\n",
    "# # -> GPU에서 AUC를 강제로 사용하려면 1) eval_metric=\"Logloss\" 2) leaf_estimation_method='Gradient' 3) devices='0'(또는 사용 GPU) 지정해야 함\n",
    "# # 참고: https://github.com/catboost/catboost/issues/1729\n",
    "\n",
    "# def find_best_auc_threshold(y_true, y_proba):\n",
    "#     \"\"\"\n",
    "#     AUC가 최대가 되도록 threshold를 탐색하여 반환\n",
    "#     \"\"\"\n",
    "#     thresholds = np.linspace(0.0, 1.0, 1001)\n",
    "#     best_auc = -np.inf\n",
    "#     best_thres = 0.5\n",
    "#     for th in thresholds:\n",
    "#         preds = (y_proba >= th).astype(int)\n",
    "#         try:\n",
    "#             auc = roc_auc_score(y_true, preds)\n",
    "#         except Exception:\n",
    "#             continue  # ROC AUC Score 계산 불가(클래스 한쪽만 나오는 경우) 시 무시\n",
    "#         if auc > best_auc:\n",
    "#             best_auc = auc\n",
    "#             best_thres = th\n",
    "#     return best_thres, best_auc\n",
    "\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         # 각 하이퍼파라미터 정의를 주석으로 추가\n",
    "#         'task_type': 'GPU',                              # 연산에 GPU 사용\n",
    "#         'iterations': trial.suggest_int('iterations', 500, 1000),                # 트리의 개수(부스팅 라운드 수)\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),  # 학습률(learning rate)\n",
    "#         'depth': trial.suggest_int('depth', 5, 8),                                # 개별 트리의 깊이\n",
    "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 30, log=True),       # L2 정규화 항의 계수\n",
    "#         'subsample': trial.suggest_float('subsample', 0.6, 0.9),                  # 훈련 데이터 샘플링 비율\n",
    "#         'random_strength': trial.suggest_int('random_strength', 1, 10),           # feature splits 선택에 randomness 적용 강도\n",
    "#         'border_count': trial.suggest_int('border_count', 32, 128),               # 연속형 변수의 bin 개수(최대 분할 수)\n",
    "#         'loss_function': 'Logloss',                                               # 손실 함수(Logloss: 이진 분류)\n",
    "#         'eval_metric': 'Logloss',                                                 # 평가 지표(Logloss 사용)\n",
    "#         'leaf_estimation_method': 'Gradient',                                     # 리프값 추정 방법\n",
    "#         'devices': '0-1',                                                           # 사용할 GPU 디바이스 (1개만 쓸 때 '0', 여러개면 '0-1' 등)\n",
    "#         'bootstrap_type': 'Bernoulli',                                            # 샘플링 방식(Bernoulli 방식)\n",
    "#         'grow_policy': 'Depthwise',                                               # 트리 성장 방식(Depthwise 방식)\n",
    "#         'early_stopping_rounds': 50,                                              # early stopping patience\n",
    "#         'verbose': 0,                                                             # 학습 중 출력 설정(0이면 출력 안 함)\n",
    "#         'gpu_ram_part': 1.0,                                                      # 전체 GPU RAM 중 사용 비율(최대값 1.0)\n",
    "#     }\n",
    "    \n",
    "#     cv_fold_aucs = []\n",
    "#     best_thresholds = []\n",
    "#     kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "#         X_fold_train = X_train.iloc[train_idx]\n",
    "#         X_fold_val = X_train.iloc[val_idx]\n",
    "#         y_fold_train = y_train.iloc[train_idx]\n",
    "#         y_fold_val = y_train.iloc[val_idx]\n",
    "\n",
    "#         model = CatBoostClassifier(**param)\n",
    "#         model.fit(\n",
    "#             X_fold_train, y_fold_train,\n",
    "#             eval_set=[(X_fold_val, y_fold_val)],\n",
    "#             verbose=0\n",
    "#         )\n",
    "\n",
    "#         val_pred_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "\n",
    "#         # AUC가 최대가 되는 threshold를 찾아 해당 threshold로 평가\n",
    "#         best_thres, best_auc = find_best_auc_threshold(y_fold_val.values, val_pred_proba)\n",
    "#         cv_fold_aucs.append(best_auc)\n",
    "#         best_thresholds.append(best_thres)\n",
    "#     mean_auc = np.mean(cv_fold_aucs)\n",
    "#     return mean_auc\n",
    "\n",
    "# # Optuna를 사용한 하이퍼파라미터 최적화 (direction='maximize'로 AUC 최대화)\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)  # 50번 시도\n",
    "\n",
    "# # 최적의 하이퍼파라미터 출력\n",
    "# print('Best trial:')\n",
    "# trial = study.best_trial\n",
    "# print('  Value: {}'.format(trial.value))\n",
    "# print('  Params: ')\n",
    "# for key, value in trial.params.items():\n",
    "#     print('    {}: {}'.format(key, value))\n",
    "\n",
    "# # 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "# best_params = study.best_params\n",
    "# best_params.update({\n",
    "#     'task_type': 'GPU',  # GPU 사용\n",
    "#     'loss_function': 'Logloss',\n",
    "#     'eval_metric': 'Logloss',  # GPU에서 AUC 평가 강제 활성화\n",
    "#     'bootstrap_type': 'Bernoulli',\n",
    "#     'grow_policy': 'Depthwise',\n",
    "#     'early_stopping_rounds': 100,\n",
    "#     'verbose': 100,\n",
    "#     'devices': '0',\n",
    "#     'leaf_estimation_method': 'Gradient',\n",
    "#     'gpu_ram_part': 0.3,\n",
    "# })\n",
    "\n",
    "# final_model = CatBoostClassifier(**best_params)\n",
    "\n",
    "# print(\"Best Params\")\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79be823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 최종적으로 검증세트 기준 best threshold로 평가 및 AUC Curve 등 시각화\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, f1_score\n",
    "\n",
    "# # 검증 세트 기준 평가 (best threshold 포함)\n",
    "# from utils.ml_pipeline import evaluate_model\n",
    "\n",
    "# # 검증 세트에서 best threshold 등 평가\n",
    "# eval_results = evaluate_model(final_model, X_test, y_test)\n",
    "\n",
    "# print(\"최종 모델 평가 결과 (Train/Validation 기준):\")\n",
    "# for key, value in eval_results.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n",
    "# # ROC Curve\n",
    "# y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "# fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# plt.figure(figsize=(7,5))\n",
    "# plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
    "# plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve (Train Set)\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()\n",
    "\n",
    "# # Best Threshold로 이진분류 및 f1/precision/recall 점수 확인\n",
    "# best_threshold = eval_results['Best Threshold']\n",
    "# y_test_pred_binary = (y_pred_proba >= best_threshold).astype(int)\n",
    "# f1 = f1_score(y_test, y_test_pred_binary)\n",
    "# precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# print(f\"Best Threshold (Train Set): {best_threshold:.4f}\")\n",
    "# print(f\"F1 Score (at best threshold): {f1:.4f}\")\n",
    "\n",
    "# # 최종 테스트 데이터 예측 및 저장\n",
    "# y_test_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "# ids = [f\"TEST_{i:05d}\" for i in range(len(X_test))]\n",
    "# result_df = pd.DataFrame({\n",
    "#     'ID': ids,\n",
    "#     'probability': y_test_pred_proba\n",
    "# })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
