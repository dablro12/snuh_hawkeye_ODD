import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
from sklearn.compose import ColumnTransformer
import warnings
warnings.filterwarnings('ignore')

# Optional: SHAP (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©)
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False


def get_original_feature_names(preprocessor, original_feature_names):
    """
    ì „ì²˜ë¦¬ëœ feature ì´ë¦„ì„ ì›ë³¸ feature ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
    
    Parameters:
    -----------
    preprocessor : ColumnTransformer
        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    original_feature_names : list
        ì›ë³¸ feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    
    Returns:
    --------
    processed_feature_names : list
        ì „ì²˜ë¦¬ëœ feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ ì´ë¦„ ë§¤í•‘ë¨)
    """
    if isinstance(preprocessor, ColumnTransformer):
        # sklearn >= 1.0: get_feature_names_out() ì‚¬ìš©
        if hasattr(preprocessor, 'get_feature_names_out'):
            try:
                processed_feature_names = preprocessor.get_feature_names_out(original_feature_names)
                return list(processed_feature_names)
            except Exception as e:
                print(f"âš ï¸  get_feature_names_out() ì‹¤íŒ¨, ìˆ˜ë™ ë§¤í•‘ ì‹œë„: {e}")
        
        # ìˆ˜ë™ ë§¤í•‘ (fallback)
        processed_feature_names = []
        feature_idx = 0
        
        for name, transformer, columns in preprocessor.transformers_:
            if name == 'remainder' and transformer == 'passthrough':
                # remainderëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ
                for col in columns:
                    if col in original_feature_names:
                        processed_feature_names.append(col)
                        feature_idx += 1
            elif transformer == 'drop':
                continue
            else:
                # ì—°ì†í˜• ë³€ìˆ˜ (StandardScaler)
                if name == 'num':
                    for col in columns:
                        processed_feature_names.append(col)
                        feature_idx += 1
                # ë²”ì£¼í˜• ë³€ìˆ˜ (OneHotEncoder)
                elif name == 'cat':
                    if hasattr(transformer, 'get_feature_names_out'):
                        # OneHotEncoderì˜ feature ì´ë¦„ ì¶”ì¶œ
                        cat_feature_names = transformer.get_feature_names_out(columns)
                        processed_feature_names.extend(cat_feature_names)
                        feature_idx += len(cat_feature_names)
                    else:
                        # sklearn < 1.0: ìˆ˜ë™ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ìƒì„±
                        for col in columns:
                            col_idx = columns.index(col) if isinstance(columns, list) else list(columns).index(col)
                            categories = transformer.categories_[col_idx]
                            # drop='first'ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸
                            for cat in categories[1:]:
                                processed_feature_names.append(f"{col}_{cat}")
                                feature_idx += 1
    else:
        # StandardScalerë§Œ ì‚¬ìš©ëœ ê²½ìš°
        processed_feature_names = original_feature_names
    
    return processed_feature_names


def map_processed_to_original_features(feature_names_processed, preprocessor, original_feature_names):
    """
    ì „ì²˜ë¦¬ëœ feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì›ë³¸ feature ì´ë¦„ìœ¼ë¡œ ë³€í™˜
    
    Parameters:
    -----------
    feature_names_processed : list
        ì „ì²˜ë¦¬ëœ feature ì´ë¦„ (ì˜ˆ: ['Feature_0', 'Feature_1', ...])
    preprocessor : ColumnTransformer
        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    original_feature_names : list
        ì›ë³¸ feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    
    Returns:
    --------
    mapped_names : list
        ì›ë³¸ feature ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ëœ ë¦¬ìŠ¤íŠ¸
    """
    processed_feature_names = get_original_feature_names(preprocessor, original_feature_names)
    
    # ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë§¤í•‘
    mapped_names = []
    for i in range(len(feature_names_processed)):
        if i < len(processed_feature_names):
            mapped_names.append(processed_feature_names[i])
        else:
            # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì´ë¦„ ìœ ì§€
            mapped_names.append(feature_names_processed[i])
    
    return mapped_names


def plot_confusion_matrix(y_true, y_pred, model_name, sampler_method=None, ax=None):
    """
    Confusion Matrix ì‹œê°í™”
    
    Parameters:
    -----------
    y_true : array
        ì‹¤ì œ íƒ€ê²Ÿ ê°’
    y_pred : array
        ì˜ˆì¸¡ ê°’
    model_name : str
        ëª¨ë¸ ì´ë¦„
    sampler_method : str, optional
        ìƒ˜í”Œë§ ë°©ë²•
    ax : matplotlib axis, optional
        subplot axis
    """
    cm = confusion_matrix(y_true, y_pred)
    
    if ax is None:
        fig, ax = plt.subplots(figsize=(8, 6))
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=['ODD_CP=0', 'ODD_CP=1'],
                yticklabels=['ODD_CP=0', 'ODD_CP=1'])
    
    title = f'Confusion Matrix - {model_name}'
    if sampler_method:
        title += f' ({sampler_method})'
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_ylabel('True Label', fontsize=12)
    ax.set_xlabel('Predicted Label', fontsize=12)
    
    # ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ê³„ì‚°
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tn + tp) / (tn + fp + fn + tp)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # í…ìŠ¤íŠ¸ ì¶”ê°€
    textstr = f'Accuracy: {accuracy:.3f}\nPrecision: {precision:.3f}\nRecall: {recall:.3f}\nF1: {f1:.3f}'
    ax.text(0.5, -0.15, textstr, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', horizontalalignment='center',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    return ax


def plot_feature_importance(model, feature_names, model_name, sampler_method=None, top_n=20, ax=None, 
                           preprocessor=None, original_feature_names=None):
    """
    Feature Importance ì‹œê°í™”
    
    Parameters:
    -----------
    model : trained model
        í•™ìŠµëœ ëª¨ë¸
    feature_names : list
        feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì „ì²˜ë¦¬ëœ)
    model_name : str
        ëª¨ë¸ ì´ë¦„
    sampler_method : str, optional
        ìƒ˜í”Œë§ ë°©ë²•
    top_n : int
        ìƒìœ„ Nê°œ featureë§Œ í‘œì‹œ
    ax : matplotlib axis, optional
        subplot axis
    preprocessor : ColumnTransformer, optional
        ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì›ë³¸ ì´ë¦„ ë§¤í•‘ìš©)
    original_feature_names : list, optional
        ì›ë³¸ feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ feature importance ì¶”ì¶œ
    importance = None
    
    if hasattr(model, 'feature_importances_'):
        # Tree-based models (RandomForest, GradientBoosting, XGBoost, LightGBM, CatBoost ë“±)
        importance = model.feature_importances_
    elif hasattr(model, 'coef_'):
        # Linear models (LogisticRegression ë“±)
        importance = np.abs(model.coef_[0])
    elif model_name == 'SVC':
        # SVCëŠ” feature importanceê°€ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
        if ax is None:
            fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, 'SVC does not provide feature importance', 
                ha='center', va='center', fontsize=14)
        ax.set_title(f'Feature Importance - {model_name}' + (f' ({sampler_method})' if sampler_method else ''))
        return ax
    else:
        # ë‹¤ë¥¸ ëª¨ë¸ë“¤ (KNN, NaiveBayes ë“±)
        if ax is None:
            fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f'{model_name} does not provide feature importance', 
                ha='center', va='center', fontsize=14)
        ax.set_title(f'Feature Importance - {model_name}' + (f' ({sampler_method})' if sampler_method else ''))
        return ax
    
    if importance is None or len(importance) == 0:
        return ax
    
    # ì›ë³¸ feature ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ (ê°€ëŠ¥í•œ ê²½ìš°)
    display_feature_names = feature_names[:len(importance)]
    if preprocessor is not None and original_feature_names is not None:
        try:
            display_feature_names = map_processed_to_original_features(
                feature_names[:len(importance)], 
                preprocessor, 
                original_feature_names
            )
        except Exception as e:
            print(f"âš ï¸  Feature ì´ë¦„ ë§¤í•‘ ì¤‘ ì˜¤ë¥˜ (ì›ë³¸ ì´ë¦„ ì‚¬ìš©): {e}")
            display_feature_names = feature_names[:len(importance)]
    
    # DataFrame ìƒì„±
    importance_df = pd.DataFrame({
        'feature': display_feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False).head(top_n)
    
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 8))
    
    # Bar plot
    colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))
    bars = ax.barh(range(len(importance_df)), importance_df['importance'], color=colors)
    ax.set_yticks(range(len(importance_df)))
    ax.set_yticklabels(importance_df['feature'])
    ax.set_xlabel('Importance', fontsize=12)
    ax.set_ylabel('Feature', fontsize=12)
    
    title = f'Feature Importance (Top {top_n}) - {model_name}'
    if sampler_method:
        title += f' ({sampler_method})'
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    # ê°’ í‘œì‹œ
    for i, (idx, row) in enumerate(importance_df.iterrows()):
        ax.text(row['importance'], i, f' {row["importance"]:.4f}', 
                va='center', fontsize=9)
    
    ax.invert_yaxis()
    plt.tight_layout()
    
    return ax, importance_df


def calculate_shap_values(model, X_train, X_test, model_name, sampler_method=None, max_samples=100):
    """
    SHAP values ê³„ì‚° ë° ì‹œê°í™”
    
    Parameters:
    -----------
    model : trained model
        í•™ìŠµëœ ëª¨ë¸
    X_train : array or DataFrame
        í•™ìŠµ ë°ì´í„°
    X_test : array or DataFrame
        í…ŒìŠ¤íŠ¸ ë°ì´í„°
    model_name : str
        ëª¨ë¸ ì´ë¦„
    sampler_method : str, optional
        ìƒ˜í”Œë§ ë°©ë²•
    max_samples : int
        SHAP ê³„ì‚°ì— ì‚¬ìš©í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
    
    Returns:
    --------
    shap_values : array
        SHAP values
    shap_explainer : explainer object
        SHAP explainer
    """
    if not SHAP_AVAILABLE:
        print("âš ï¸  SHAPì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install shap ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return None, None
    
    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    if len(X_test) > max_samples:
        np.random.seed(42)
        sample_idx = np.random.choice(len(X_test), max_samples, replace=False)
        X_test_sample = X_test[sample_idx] if isinstance(X_test, np.ndarray) else X_test.iloc[sample_idx]
    else:
        X_test_sample = X_test
    
    try:
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ explainer ì„ íƒ
        if model_name in ['XGBoost', 'LightGBM', 'CatBoost', 'RandomForest', 
                          'GradientBoosting', 'ExtraTrees', 'DecisionTree', 'AdaBoost']:
            explainer = shap.TreeExplainer(model)
        elif model_name == 'LogisticRegression':
            explainer = shap.LinearExplainer(model, X_train[:100])  # ìƒ˜í”Œë§
        elif model_name == 'SVC':
            explainer = shap.KernelExplainer(model.predict_proba, X_train[:100])
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ KernelExplainer ì‚¬ìš©
            explainer = shap.KernelExplainer(model.predict_proba, X_train[:100])
        
        shap_values = explainer.shap_values(X_test_sample)
        
        # Binary classificationì¸ ê²½ìš° shap_valuesê°€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
        if isinstance(shap_values, list):
            shap_values = shap_values[1]  # í´ë˜ìŠ¤ 1ì— ëŒ€í•œ SHAP values
        
        return shap_values, explainer, X_test_sample
        
    except Exception as e:
        print(f"âš ï¸  SHAP ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None


def plot_shap_summary(shap_values, X_test_sample, feature_names, model_name, sampler_method=None, max_display=20):
    """
    SHAP Summary Plot
    
    Parameters:
    -----------
    shap_values : array
        SHAP values
    X_test_sample : array or DataFrame
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ
    feature_names : list
        feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    model_name : str
        ëª¨ë¸ ì´ë¦„
    sampler_method : str, optional
        ìƒ˜í”Œë§ ë°©ë²•
    max_display : int
        ìµœëŒ€ í‘œì‹œ feature ìˆ˜
    """
    if shap_values is None:
        return
    
    if isinstance(X_test_sample, pd.DataFrame):
        X_test_sample = X_test_sample.values
    
    # Feature names ì„¤ì •
    if len(feature_names) != X_test_sample.shape[1]:
        feature_names = [f'Feature_{i}' for i in range(X_test_sample.shape[1])]
    
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_test_sample, 
                     feature_names=feature_names[:X_test_sample.shape[1]],
                     max_display=max_display, show=False)
    
    title = f'SHAP Summary Plot - {model_name}'
    if sampler_method:
        title += f' ({sampler_method})'
    plt.title(title, fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.show()


def plot_shap_bar(shap_values, feature_names, model_name, sampler_method=None, max_display=20):
    """
    SHAP Bar Plot (í‰ê·  ì ˆëŒ€ê°’)
    
    Parameters:
    -----------
    shap_values : array
        SHAP values
    feature_names : list
        feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    model_name : str
        ëª¨ë¸ ì´ë¦„
    sampler_method : str, optional
        ìƒ˜í”Œë§ ë°©ë²•
    max_display : int
        ìµœëŒ€ í‘œì‹œ feature ìˆ˜
    """
    if shap_values is None:
        return
    
    # í‰ê·  ì ˆëŒ€ê°’ ê³„ì‚°
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    
    # DataFrame ìƒì„±
    shap_df = pd.DataFrame({
        'feature': feature_names[:len(mean_abs_shap)],
        'mean_abs_shap': mean_abs_shap
    }).sort_values('mean_abs_shap', ascending=False).head(max_display)
    
    # Plot
    fig, ax = plt.subplots(figsize=(10, 8))
    colors = plt.cm.plasma(np.linspace(0, 1, len(shap_df)))
    bars = ax.barh(range(len(shap_df)), shap_df['mean_abs_shap'], color=colors)
    ax.set_yticks(range(len(shap_df)))
    ax.set_yticklabels(shap_df['feature'])
    ax.set_xlabel('Mean |SHAP value|', fontsize=12)
    ax.set_ylabel('Feature', fontsize=12)
    
    title = f'SHAP Feature Importance (Top {max_display}) - {model_name}'
    if sampler_method:
        title += f' ({sampler_method})'
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    # ê°’ í‘œì‹œ
    for i, (idx, row) in enumerate(shap_df.iterrows()):
        ax.text(row['mean_abs_shap'], i, f' {row["mean_abs_shap"]:.4f}', 
                va='center', fontsize=9)
    
    ax.invert_yaxis()
    plt.tight_layout()
    plt.show()
    
    return shap_df


def analyze_best_model(sampler_result_dict, sampler_results, trainer_dict, 
                      X_test_orig, y_test_orig, feature_names, 
                      plot_confusion=True, plot_importance=True, plot_shap=True):
    """
    ê° ìƒ˜í”Œë§ ë°©ë²•ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¶„ì„
    
    Parameters:
    -----------
    sampler_result_dict : dict
        ìƒ˜í”Œë§ ë°©ë²•ë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    sampler_results : dict
        ìƒ˜í”Œë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    trainer_dict : dict
        ìƒ˜í”Œë§ ë°©ë²•ë³„ trainer ë”•ì…”ë„ˆë¦¬
    X_test_orig : DataFrame
        ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    y_test_orig : Series
        ì›ë³¸ í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ
    feature_names : list
        ì›ë³¸ feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    plot_confusion : bool
        Confusion Matrix í”Œë¡¯ ì—¬ë¶€
    plot_importance : bool
        Feature Importance í”Œë¡¯ ì—¬ë¶€
    plot_shap : bool
        SHAP í”Œë¡¯ ì—¬ë¶€
    """
    results_summary = []
    
    for method in sampler_result_dict.keys():
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ìƒ˜í”Œë§ ë°©ë²•: {method.upper()}")
        print(f"{'='*60}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸° (F1 ê¸°ì¤€)
        results_df = sampler_result_dict[method]
        best_model_row = results_df.loc[results_df['F1'].idxmax()]
        best_model_name = best_model_row['Model']
        best_model = best_model_row['model']
        
        print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        print(f"  Accuracy: {best_model_row['Accuracy']:.4f}")
        print(f"  Precision: {best_model_row['Precision']:.4f}")
        print(f"  Recall: {best_model_row['Recall']:.4f}")
        print(f"  F1: {best_model_row['F1']:.4f}")
        
        # ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°
        y_pred = best_model_row['cls_df']['y_pred'].values
        
        # Trainerì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        trainer = trainer_dict[method]
        X_test_processed = trainer.X_test_processed
        X_train_processed = trainer.X_train_processed
        
        # 1. Confusion Matrix
        if plot_confusion:
            print("\nğŸ“‹ Confusion Matrix:")
            fig, ax = plt.subplots(figsize=(8, 6))
            plot_confusion_matrix(y_test_orig, y_pred, best_model_name, method, ax)
            plt.show()
        
        # 2. Feature Importance
        if plot_importance:
            print("\nğŸ“Š Feature Importance:")
            try:
                # ì „ì²˜ë¦¬ëœ feature ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                processed_feature_names = [f'Feature_{i}' for i in range(X_test_processed.shape[1])]
                fig, ax = plt.subplots(figsize=(10, 8))
                ax, importance_df = plot_feature_importance(
                    best_model, processed_feature_names, best_model_name, 
                    method, top_n=20, ax=ax,
                    preprocessor=trainer.preprocessor,  # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì „ë‹¬
                    original_feature_names=feature_names  # ì›ë³¸ feature ì´ë¦„ ì „ë‹¬
                )
                plt.show()
                print(f"\nìƒìœ„ 10ê°œ ì¤‘ìš” ë³€ìˆ˜:")
                print(importance_df.head(10)[['feature', 'importance']].to_string(index=False))
            except Exception as e:
                print(f"âš ï¸  Feature Importance ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
        
        # 3. SHAP Values
        if plot_shap:
            print("\nğŸ” SHAP Values:")
            try:
                shap_values, explainer, X_test_sample = calculate_shap_values(
                    best_model, X_train_processed, X_test_processed, 
                    best_model_name, method, max_samples=100
                )
                
                if shap_values is not None:
                    processed_feature_names = [f'Feature_{i}' for i in range(X_test_processed.shape[1])]
                    
                    # ì›ë³¸ feature ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
                    try:
                        mapped_feature_names = map_processed_to_original_features(
                            processed_feature_names,
                            trainer.preprocessor,
                            feature_names
                        )
                    except Exception as e:
                        print(f"âš ï¸  SHAP Feature ì´ë¦„ ë§¤í•‘ ì¤‘ ì˜¤ë¥˜ (ì›ë³¸ ì´ë¦„ ì‚¬ìš©): {e}")
                        mapped_feature_names = processed_feature_names
                    
                    # SHAP Summary Plot
                    plot_shap_summary(shap_values, X_test_sample, 
                                    mapped_feature_names, best_model_name, method)
                    
                    # SHAP Bar Plot
                    shap_df = plot_shap_bar(shap_values, mapped_feature_names, 
                                           best_model_name, method)
                    print(f"\nìƒìœ„ 10ê°œ SHAP ì¤‘ìš” ë³€ìˆ˜:")
                    print(shap_df.head(10)[['feature', 'mean_abs_shap']].to_string(index=False))
            except Exception as e:
                print(f"âš ï¸  SHAP ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        
        results_summary.append({
            'sampler_method': method,
            'best_model': best_model_name,
            'accuracy': best_model_row['Accuracy'],
            'precision': best_model_row['Precision'],
            'recall': best_model_row['Recall'],
            'f1': best_model_row['F1']
        })
    
    # ì „ì²´ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“Š ì „ì²´ ìš”ì•½")
    print(f"{'='*60}")
    summary_df = pd.DataFrame(results_summary)
    print(summary_df.to_string(index=False))
    
    return summary_df

