"""
ë°ì´í„° ì „ì²˜ë¦¬ ê²€í†  ë° ê¶Œì¥ì‚¬í•­ ì œê³µ
"""
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt


def check_preprocessing_needs(df: pd.DataFrame, target_col: str = None) -> Dict:
    """
    ë°ì´í„° ì „ì²˜ë¦¬ í•„ìš”ì‚¬í•­ ê²€í† 
    
    Parameters
    ----------
    df : DataFrame
        ê²€í† í•  ë°ì´í„°í”„ë ˆì„
    target_col : str, optional
        íƒ€ê²Ÿ ë³€ìˆ˜ëª… (ì œê³µë˜ë©´ íƒ€ê²Ÿê³¼ì˜ ê´€ê³„ë„ ë¶„ì„)
    
    Returns
    -------
    dict
        ì „ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
    """
    recommendations = {
        'weight_variables': [],
        'categorical_to_convert': [],
        'high_cardinality_categorical': [],
        'low_variance_features': [],
        'missing_values': {},
        'outliers': {},
        'data_leakage_risk': [],
        'recommendations': []
    }
    
    print("=" * 70)
    print("ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ê²€í†  ë¦¬í¬íŠ¸")
    print("=" * 70)
    
    # 1. ê°€ì¤‘ì¹˜ ë³€ìˆ˜ í™•ì¸
    print("\n1ï¸âƒ£ ê°€ì¤‘ì¹˜ ë³€ìˆ˜ í™•ì¸")
    print("-" * 70)
    weight_cols = [col for col in df.columns if 'wt' in col.lower() or 'weight' in col.lower()]
    if weight_cols:
        recommendations['weight_variables'] = weight_cols
        print(f"âš ï¸  ë°œê²¬ëœ ê°€ì¤‘ì¹˜ ë³€ìˆ˜: {weight_cols}")
        print("   â†’ ê°€ì¤‘ì¹˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ ì‹œ sample_weightë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì œê±°í•©ë‹ˆë‹¤.")
        print("   â†’ íŠ¹ì„±ìœ¼ë¡œ í¬í•¨í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì— ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        recommendations['recommendations'].append({
            'type': 'weight',
            'action': 'ì œê±° ë˜ëŠ” sample_weightë¡œ ì‚¬ìš©',
            'columns': weight_cols
        })
    else:
        print("âœ… ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì—†ìŒ")
    
    # 2. ë²”ì£¼í˜• ë³€ìˆ˜ íƒ€ì… í™•ì¸
    print("\n2ï¸âƒ£ ë²”ì£¼í˜• ë³€ìˆ˜ íƒ€ì… í™•ì¸")
    print("-" * 70)
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    print(f"ë²”ì£¼í˜• ë³€ìˆ˜ (object): {len(categorical_cols)}ê°œ")
    if categorical_cols:
        for col in categorical_cols:
            n_unique = df[col].nunique()
            print(f"  - {col}: {n_unique}ê°œ ê³ ìœ ê°’")
            if n_unique > 20:
                recommendations['high_cardinality_categorical'].append({
                    'column': col,
                    'n_unique': n_unique
                })
                print(f"    âš ï¸  ê³ ìœ ê°’ì´ ë§ìŒ ({n_unique}ê°œ) - OneHotEncoding ì‹œ ì°¨ì› ì¦ê°€ ì£¼ì˜")
    
    # 3. ìˆ«ìí˜•ì´ì§€ë§Œ ë²”ì£¼í˜•ì¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë³€ìˆ˜ í™•ì¸
    print(f"\nìˆ«ìí˜• ë³€ìˆ˜ ì¤‘ ë²”ì£¼í˜•ì¼ ê°€ëŠ¥ì„± ìˆëŠ” ë³€ìˆ˜:")
    potential_categorical = []
    for col in numeric_cols:
        n_unique = df[col].nunique()
        if n_unique <= 10 and n_unique < len(df) * 0.1:  # ê³ ìœ ê°’ì´ 10ê°œ ì´í•˜ì´ê³  ì „ì²´ì˜ 10% ë¯¸ë§Œ
            potential_categorical.append({
                'column': col,
                'n_unique': n_unique,
                'values': sorted(df[col].unique().tolist())
            })
            print(f"  - {col}: {n_unique}ê°œ ê³ ìœ ê°’ {sorted(df[col].unique().tolist())}")
    
    if potential_categorical:
        recommendations['categorical_to_convert'] = potential_categorical
        recommendations['recommendations'].append({
            'type': 'categorical_conversion',
            'action': 'ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜ ê³ ë ¤',
            'columns': [d['column'] for d in potential_categorical]
        })
    
    # 4. ê²°ì¸¡ì¹˜ í™•ì¸
    print("\n3ï¸âƒ£ ê²°ì¸¡ì¹˜ í™•ì¸")
    print("-" * 70)
    missing = df.isnull().sum()
    missing_cols = missing[missing > 0]
    if len(missing_cols) > 0:
        recommendations['missing_values'] = missing_cols.to_dict()
        print("âš ï¸  ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜:")
        for col, count in missing_cols.items():
            pct = count / len(df) * 100
            print(f"  - {col}: {count}ê°œ ({pct:.2f}%)")
            recommendations['recommendations'].append({
                'type': 'missing',
                'action': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•„ìš”',
                'column': col,
                'count': count,
                'percentage': pct
            })
    else:
        print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
    
    # 5. ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ í™•ì¸ (ê±°ì˜ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ê²½ìš°)
    print("\n4ï¸âƒ£ ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ í™•ì¸")
    print("-" * 70)
    low_variance = []
    for col in numeric_cols:
        if df[col].nunique() == 1:
            low_variance.append(col)
            print(f"  âš ï¸  {col}: ëª¨ë“  ê°’ì´ ë™ì¼ (ì œê±° ê¶Œì¥)")
        elif df[col].nunique() == 2:
            # ì´ì§„ ë³€ìˆ˜ì¸ ê²½ìš°, í•œ í´ë˜ìŠ¤ê°€ 95% ì´ìƒì´ë©´ ë‚®ì€ ë¶„ì‚°ìœ¼ë¡œ ê°„ì£¼
            value_counts = df[col].value_counts(normalize=True)
            if value_counts.max() >= 0.95:
                low_variance.append(col)
                print(f"  âš ï¸  {col}: í•œ ê°’ì´ {value_counts.max()*100:.1f}% ì°¨ì§€ (ì œê±° ê³ ë ¤)")
    
    recommendations['low_variance_features'] = low_variance
    if low_variance:
        recommendations['recommendations'].append({
            'type': 'low_variance',
            'action': 'ì œê±° ê³ ë ¤',
            'columns': low_variance
        })
    else:
        print("âœ… ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ ì—†ìŒ")
    
    # 6. ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ë³€ìˆ˜ í™•ì¸ (íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°)
    if target_col and target_col in df.columns:
        print("\n5ï¸âƒ£ ë°ì´í„° ëˆ„ìˆ˜(Data Leakage) ìœ„í—˜ ë³€ìˆ˜ í™•ì¸")
        print("-" * 70)
        y = df[target_col]
        leakage_risk = []
        
        for col in numeric_cols:
            if col == target_col:
                continue
            # ì¼ì¹˜ìœ¨ í™•ì¸
            if df[col].dtype in [np.int64, np.int32]:
                match_rate = (df[col] == y).mean()
                if match_rate >= 0.95:
                    leakage_risk.append({
                        'column': col,
                        'match_rate': match_rate,
                        'reason': 'íƒ€ê²Ÿê³¼ 95% ì´ìƒ ì¼ì¹˜'
                    })
                    print(f"  âš ï¸  {col}: íƒ€ê²Ÿê³¼ {match_rate*100:.1f}% ì¼ì¹˜")
            
            # ìƒê´€ê´€ê³„ í™•ì¸
            corr = abs(df[col].corr(y))
            if corr >= 0.9:
                if col not in [r['column'] for r in leakage_risk]:
                    leakage_risk.append({
                        'column': col,
                        'correlation': corr,
                        'reason': 'íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ 0.9 ì´ìƒ'
                    })
                    print(f"  âš ï¸  {col}: íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ {corr:.4f}")
        
        recommendations['data_leakage_risk'] = leakage_risk
        if leakage_risk:
            recommendations['recommendations'].append({
                'type': 'data_leakage',
                'action': 'ì œê±° í•„ìˆ˜',
                'columns': [r['column'] for r in leakage_risk]
            })
        else:
            print("âœ… ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ë³€ìˆ˜ ì—†ìŒ")
    
    # 7. ì´ìƒì¹˜ í™•ì¸ (IQR ë°©ë²•)
    print("\n6ï¸âƒ£ ì´ìƒì¹˜ í™•ì¸ (IQR ë°©ë²•)")
    print("-" * 70)
    outlier_summary = {}
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        if IQR > 0:  # IQRì´ 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            if outliers > 0:
                outlier_pct = outliers / len(df) * 100
                outlier_summary[col] = {
                    'count': outliers,
                    'percentage': outlier_pct,
                    'lower_bound': lower_bound,
                    'upper_bound': upper_bound
                }
                if outlier_pct > 5:  # ì´ìƒì¹˜ê°€ 5% ì´ìƒì¸ ê²½ìš°ë§Œ ê²½ê³ 
                    print(f"  âš ï¸  {col}: {outliers}ê°œ ({outlier_pct:.2f}%) ì´ìƒì¹˜")
    
    recommendations['outliers'] = outlier_summary
    if not outlier_summary:
        print("âœ… ì´ìƒì¹˜ê°€ ë§ì€ ë³€ìˆ˜ ì—†ìŒ (5% ê¸°ì¤€)")
    
    # 8. ì¢…í•© ê¶Œì¥ì‚¬í•­
    print("\n" + "=" * 70)
    print("ğŸ“‹ ì¢…í•© ê¶Œì¥ì‚¬í•­")
    print("=" * 70)
    
    if not recommendations['recommendations']:
        print("âœ… íŠ¹ë³„í•œ ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   ClassifierTrainerê°€ ìë™ìœ¼ë¡œ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ OneHotEncodingí•˜ê³ ,")
        print("   ì—°ì†í˜• ë³€ìˆ˜ë¥¼ StandardScalerë¡œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.")
    else:
        for i, rec in enumerate(recommendations['recommendations'], 1):
            print(f"\n{i}. {rec['type'].upper()}: {rec['action']}")
            if 'columns' in rec:
                print(f"   ëŒ€ìƒ ë³€ìˆ˜: {rec['columns']}")
            elif 'column' in rec:
                print(f"   ëŒ€ìƒ ë³€ìˆ˜: {rec['column']}")
    
    print("\n" + "=" * 70)
    
    return recommendations


def preprocess_dataframe(
    df: pd.DataFrame,
    target_col: str = None,
    drop_weight: bool = True,
    convert_categorical: list = None,
    convert_ordinal: list = None,
    convert_binary: list = None,
    drop_low_variance: bool = False,
    drop_leakage: bool = True
) -> pd.DataFrame:
    """
    ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ ì‹¤í–‰

    Parameters
    ----------
    df : DataFrame
        ì „ì²˜ë¦¬í•  ë°ì´í„°í”„ë ˆì„
    target_col : str, optional
        íƒ€ê²Ÿ ë³€ìˆ˜ëª…
    drop_weight : bool
        ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì œê±° ì—¬ë¶€ (default=True)
    convert_categorical : list, optional
        ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    convert_ordinal : list, optional
        ìˆœì„œí˜•ìœ¼ë¡œ ë³€í™˜í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    convert_binary : list, optional
        ì´ì§„í˜•(0/1)ìœ¼ë¡œ ë³€í™˜í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    drop_low_variance : bool
        ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ ì œê±° ì—¬ë¶€ (default=False)
    drop_leakage : bool
        ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ë³€ìˆ˜ ì œê±° ì—¬ë¶€ (default=True)

    Returns
    -------
    DataFrame
        ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    import pandas as pd

    df_processed = df.copy()

    # 1. ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì œê±°
    if drop_weight:
        weight_cols = [col for col in df_processed.columns if 'wt' in col.lower() or 'weight' in col.lower()]
        if weight_cols:
            df_processed = df_processed.drop(columns=weight_cols)
            print(f"âœ… ê°€ì¤‘ì¹˜ ë³€ìˆ˜ ì œê±°: {weight_cols}")

    # 2. ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜
    if convert_categorical:
        for col in convert_categorical:
            if col in df_processed.columns:
                df_processed[col] = df_processed[col].astype('category')
                # print(f"âœ… {col}ì„ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜")

    # 2-1. ìˆœì„œí˜•ìœ¼ë¡œ ë³€í™˜
    if convert_ordinal:
        for col in convert_ordinal:
            if col in df_processed.columns:
                df_processed[col] = df_processed[col].astype('category')
                df_processed[col] = df_processed[col].cat.as_ordered()
                # print(f"âœ… {col}ì„ ìˆœì„œí˜•(category, ordered)ìœ¼ë¡œ ë³€í™˜")

    # 2-2. ì´ì§„í˜•ìœ¼ë¡œ ë³€í™˜ (0ê³¼ 1ë¡œ ë§¤í•‘)
    if convert_binary:
        for col in convert_binary:
            if col in df_processed.columns:
                unique_vals = sorted(df_processed[col].dropna().unique())
                if len(unique_vals) == 2:
                    bin_map = {unique_vals[0]: 0, unique_vals[1]: 1}
                    df_processed[col] = df_processed[col].map(bin_map)
                else:
                    # ì´ë¯¸ 0/1ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì—ëŸ¬ ì¶œë ¥
                    print(f"âš ï¸  {col} ë³€ìˆ˜ëŠ” 2ê°œì˜ ê°’ì´ ì•„ë‹™ë‹ˆë‹¤: {unique_vals}")
                # print(f"âœ… {col}ì„ ì´ì§„í˜•(0/1)ìœ¼ë¡œ ë³€í™˜")

    # 3. ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ ì œê±°
    if drop_low_variance:
        import numpy as np
        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
        low_variance_cols = []
        for col in numeric_cols:
            if col == target_col:
                continue
            if df_processed[col].nunique() == 1:
                low_variance_cols.append(col)
            elif df_processed[col].nunique() == 2:
                value_counts = df_processed[col].value_counts(normalize=True)
                if value_counts.max() >= 0.95:
                    low_variance_cols.append(col)

        if low_variance_cols:
            df_processed = df_processed.drop(columns=low_variance_cols)
            print(f"âœ… ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ ì œê±°: {low_variance_cols}")

    # 4. ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ë³€ìˆ˜ ì œê±°
    if drop_leakage and target_col and target_col in df_processed.columns:
        recommendations = check_preprocessing_needs(df_processed, target_col)
        leakage_cols = [r['column'] for r in recommendations['data_leakage_risk']]
        if leakage_cols:
            df_processed = df_processed.drop(columns=leakage_cols)
            print(f"âœ… ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ ë³€ìˆ˜ ì œê±°: {leakage_cols}")

    return df_processed



import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler

def categorical_preprocess(df):
    """
    ìˆœì„œê°€ ì—†ëŠ” ë²”ì£¼í˜• (category dtype, ordered=False) ì²˜ë¦¬: 
    - ê²°ì¸¡ì¹˜ì²˜ë¦¬ : 9999ë¡œ ëŒ€ì²´ 
    - OneHotEncoding
    """
    df = df.copy()
    # category dtype, ordered=False ë§Œ ì„ íƒ
    cat_cols = [c for c in df.select_dtypes(['category']).columns
                if not df[c].cat.ordered]
    # ê²°ì¸¡ê°’ì„ 9999ë¡œ ëŒ€ì²´
    for col in cat_cols:
        # category dtypeì—ì„œëŠ” ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ë¥¼ ë¨¼ì € ì¶”ê°€í•´ì•¼ í•¨
        if '9999' not in df[col].cat.categories:
            df[col] = df[col].cat.add_categories('9999')
        df[col] = df[col].fillna('9999')
        # OneHotEncoderë¥¼ ìœ„í•´ ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (íƒ€ì… í†µì¼)
        df[col] = df[col].astype(str)
    if cat_cols:
        encoder = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
        encoded = encoder.fit_transform(df[cat_cols])
        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_cols), index=df.index)
        # ê¸°ì¡´ categorical ì»¬ëŸ¼ ì œê±° í›„ ê²°í•©
        df = df.drop(columns=cat_cols)
        df = pd.concat([df, encoded_df], axis=1)
    return df

def ordinal_categorical_preprocess(df):
    """
    ìˆœì„œê°€ ìˆëŠ” category ì²˜ë¦¬: category dtype, ordered=True
    - NaNì²˜ë¦¬ : Medianê°’
    - pandasì˜ category ì½”ë“œê°’(int)ë¡œ ë³€í™˜
    """
    df = df.copy()
    ord_cols = [c for c in df.select_dtypes(['category']).columns
                if df[c].cat.ordered]
    for col in ord_cols:
        # ì¹´í…Œê³ ë¦¬ ì½”ë“œê°’(int)ë¡œ ë³€í™˜ (ì½ê¸° ì „ìš©ì´ë¯€ë¡œ ë³µì‚¬ë³¸ ìƒì„±)
        codes = df[col].cat.codes.copy()
        # ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§„ ì¸ë±ìŠ¤ ì°¾ê¸° (cat.codesì—ì„œ ê²°ì¸¡ì¹˜ëŠ” -1)
        nan_idx = codes[codes == -1].index
        # ê²°ì¸¡ì¹˜ê°€ ìˆì„ ë•Œ median codeë¡œ ëŒ€ì²´
        if len(nan_idx) > 0:
            valid_codes = codes[codes != -1]
            if len(valid_codes) > 0:
                median_code = int(np.median(valid_codes))
                codes.loc[nan_idx] = median_code
            else:
                # ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜ì¸ ê²½ìš° 0ìœ¼ë¡œ ëŒ€ì²´
                codes.loc[nan_idx] = 0
        df[col] = codes
    return df

def object_preprocess(df):
    """
    object dtype ì²˜ë¦¬: 
    - NaNì„ "Unknown"ìœ¼ë¡œ ëŒ€ì²´(ì´ìƒì¹˜ê°’) í›„, OrdinalEncoder
    """
    df = df.copy()
    obj_cols = df.select_dtypes(include=["object"]).columns
    if len(obj_cols) == 0:
        return df
    df[obj_cols] = df[obj_cols].fillna("Unknown")
    encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
    df[obj_cols] = encoder.fit_transform(df[obj_cols])
    return df

def integer_preprocess(df):
    """ 
    int dtype: NaNì€ medianìœ¼ë¡œ ëŒ€ì²´ í›„ Z-í‘œì¤€í™”(StandardScaler) ì ìš©
    """
    df = df.copy()
    int_cols = df.select_dtypes(include=["int", "int64"]).columns
    if len(int_cols):
        median_vals = df[int_cols].median()
        df[int_cols] = df[int_cols].fillna(median_vals)
        scaler = StandardScaler()
        df[int_cols] = scaler.fit_transform(df[int_cols])
    return df

def float_preprocess(df):
    """
    float dtype: NaNì€ medianìœ¼ë¡œ ëŒ€ì²´ + í‘œì¤€í™”(StandardScaler)
    """
    df = df.copy()
    float_cols = df.select_dtypes(include=["float", "float64"]).columns
    if len(float_cols):
        median_vals = df[float_cols].median()
        df[float_cols] = df[float_cols].fillna(median_vals)
        scaler = StandardScaler()
        df[float_cols] = scaler.fit_transform(df[float_cols])
    return df

def data_preprocess_pipeline(df):
    """
    dtypeë³„ ì „ì²˜ë¦¬ í†µí•© íŒŒì´í”„ë¼ì¸: 
      1. ìˆœì„œ ìˆëŠ” category (ordinal) â†’ ordinal_categorical_preprocess (NaNì€ Median)
      2. object â†’ object_preprocess (NaNì€ Unknown)
      3. ìˆœì„œ ì—†ëŠ” category â†’ categorical_preprocess (NaNì€ Unknown)
      4. int â†’ integer_preprocess (NaNì€ Median)
      5. float â†’ float_preprocess (NaNì€ Median)
    """

    print("â–¶ integer ì „ì²˜ë¦¬ ì¤‘...")
    df = integer_preprocess(df)

    print("â–¶ float ì „ì²˜ë¦¬ ì¤‘...")
    df = float_preprocess(df)
    
    print("â–¶ ìˆœì„œ ìˆëŠ” category(ordinal) ì „ì²˜ë¦¬ ì¤‘...")
    df = ordinal_categorical_preprocess(df)

    print("â–¶ object ì „ì²˜ë¦¬ ì¤‘...")
    df = object_preprocess(df)

    print("â–¶ ìˆœì„œ ì—†ëŠ” category ì „ì²˜ë¦¬ ì¤‘...")
    df = categorical_preprocess(df)


    print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
    return df
