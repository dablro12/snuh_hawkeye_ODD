from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import numpy as np
from tqdm import tqdm

# Optional: XGBoost, LightGBM, CatBoost (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False

class ClassifierTrainer:
    """
    Binary Classification Trainer for ODD_CP prediction
    X: ì…ë ¥ feature (DataFrame ë˜ëŠ” array) - ì—°ì†í˜•ê³¼ ë²”ì£¼í˜• ë³€ìˆ˜ ëª¨ë‘ í¬í•¨ ê°€ëŠ¥
    y: Binary target (0 ë˜ëŠ” 1)
    """
    def __init__(self, X, y, X_test=None, y_test=None, random_state=42, n_estimators=500, epoch=100, lr=0.05, n_jobs=-1):
        """
        Parameters:
        -----------
        X : DataFrame or array
            í•™ìŠµìš© feature (ë˜ëŠ” ì „ì²´ ë°ì´í„°)
        y : Series or array
            í•™ìŠµìš© íƒ€ê²Ÿ ë³€ìˆ˜ (ë˜ëŠ” ì „ì²´ ë°ì´í„°)
        X_test : DataFrame or array, optional
            í…ŒìŠ¤íŠ¸ìš© feature (ì œê³µë˜ë©´ XëŠ” train setìœ¼ë¡œ ê°„ì£¼)
        y_test : Series or array, optional
            í…ŒìŠ¤íŠ¸ìš© íƒ€ê²Ÿ ë³€ìˆ˜ (ì œê³µë˜ë©´ yëŠ” train setìœ¼ë¡œ ê°„ì£¼)
        """
        self.n_estimators = n_estimators
        self.epoch = epoch
        self.lr = lr
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.device = "cpu"
        
        # Xê°€ DataFrameì¸ì§€ í™•ì¸
        if isinstance(X, pd.DataFrame):
            self.X_df = X.copy()
            self.feature_names = X.columns.tolist()
        else:
            # numpy arrayì¸ ê²½ìš° DataFrameìœ¼ë¡œ ë³€í™˜
            self.X_df = pd.DataFrame(X)
            self.feature_names = [f'feature_{i}' for i in range(X.shape[1])]
            self.X_df.columns = self.feature_names
        
        # yë¥¼ numpy arrayë¡œ ë³€í™˜ (binary: 0 ë˜ëŠ” 1)
        self.y = np.asarray(y).astype(int)
        
        # Binary classification í™•ì¸
        unique_classes = np.unique(self.y)
        if len(unique_classes) != 2 or not all(c in [0, 1] for c in unique_classes):
            raise ValueError(f"yëŠ” binary classificationì´ì–´ì•¼ í•©ë‹ˆë‹¤ (0 ë˜ëŠ” 1). í˜„ì¬ í´ë˜ìŠ¤: {unique_classes}")
        
        # Test setì´ ì œê³µëœ ê²½ìš° (ì´ë¯¸ ë¶„ë¦¬ëœ ê²½ìš°)
        if X_test is not None and y_test is not None:
            if isinstance(X_test, pd.DataFrame):
                self.X_test = X_test.copy()
            else:
                self.X_test = pd.DataFrame(X_test, columns=self.feature_names)
            self.y_test = np.asarray(y_test).astype(int)
            self.X_train = self.X_df
            self.y_train = self.y
            
            print(f"Train íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬: {np.bincount(self.y_train)} (í´ë˜ìŠ¤ 0: {np.sum(self.y_train==0)}, í´ë˜ìŠ¤ 1: {np.sum(self.y_train==1)})")
            print(f"Test íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬: {np.bincount(self.y_test)} (í´ë˜ìŠ¤ 0: {np.sum(self.y_test==0)}, í´ë˜ìŠ¤ 1: {np.sum(self.y_test==1)})")
        else:
            # Test setì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°: ìë™ìœ¼ë¡œ ë¶„ë¦¬
            print(f"íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬: {np.bincount(self.y)} (í´ë˜ìŠ¤ 0: {np.sum(self.y==0)}, í´ë˜ìŠ¤ 1: {np.sum(self.y==1)})")
            
            # ë°ì´í„° ë¶„ë¦¬
            (
                self.X_train, self.X_test,
                self.y_train, self.y_test
            ) = train_test_split(
                self.X_df, self.y, test_size=0.2,
                random_state=self.random_state,
                stratify=self.y
            )
            print(f"ë°ì´í„° ìë™ ë¶„ë¦¬ ì™„ë£Œ (Train/Test: {len(self.X_train)} / {len(self.X_test)})")
        
        # ë°ì´í„° ëˆ„ìˆ˜(Data Leakage) ê°ì§€ (train setì— ëŒ€í•´ì„œë§Œ)
        self._detect_data_leakage()
        
        # ë²”ì£¼í˜•/ì—°ì†í˜• ë³€ìˆ˜ êµ¬ë¶„
        self._identify_column_types()
        
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
        self._build_preprocessor()
        
        # ì „ì²˜ë¦¬ ì ìš© (trainìœ¼ë¡œ fit, testë¡œ transform)
        self.X_train_processed = self.preprocessor.fit_transform(self.X_train)
        self.X_test_processed = self.preprocessor.transform(self.X_test)
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ âœ…")
        print(f"  - Train/Test: {len(self.X_train)} / {len(self.X_test)}")
        print(f"  - ì›ë³¸ feature ìˆ˜: {len(self.feature_names)}")
        print(f"  - ì „ì²˜ë¦¬ í›„ feature ìˆ˜: {self.X_train_processed.shape[1]}")
        print(f"  - ë²”ì£¼í˜• ë³€ìˆ˜: {len(self.categorical_cols)}ê°œ")
        print(f"  - ì—°ì†í˜• ë³€ìˆ˜: {len(self.numeric_cols)}ê°œ")
        print("-" * 60)
    
    def _detect_data_leakage(self):
        """ë°ì´í„° ëˆ„ìˆ˜(Data Leakage) ë³€ìˆ˜ ê°ì§€ (train setì— ëŒ€í•´ì„œë§Œ)"""
        leakage_vars = []
        numeric_cols = self.X_train.select_dtypes(include=[np.number]).columns.tolist()
        
        for col in numeric_cols:
            # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ì¼ì¹˜ìœ¨ ê³„ì‚° (train set ê¸°ì¤€)
            match_rate = (self.X_train[col] == self.y_train).mean()
            # ìƒê´€ê´€ê³„ ê³„ì‚°
            corr = abs(self.X_train[col].corr(pd.Series(self.y_train)))
            
            # ì¼ì¹˜ìœ¨ì´ 95% ì´ìƒì´ê±°ë‚˜ ìƒê´€ê´€ê³„ê°€ 0.9 ì´ìƒì´ë©´ ê²½ê³ 
            if match_rate >= 0.95 or corr >= 0.9:
                leakage_vars.append({
                    'variable': col,
                    'match_rate': match_rate,
                    'correlation': corr
                })
        
        if leakage_vars:
            print("\nâš ï¸  ë°ì´í„° ëˆ„ìˆ˜(Data Leakage) ê²½ê³ :")
            print("ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ íƒ€ê²Ÿ ë³€ìˆ˜ì™€ ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:")
            for var in leakage_vars:
                print(f"  - {var['variable']}: ì¼ì¹˜ìœ¨ {var['match_rate']:.2%}, ìƒê´€ê´€ê³„ {var['correlation']:.4f}")
            print("ì´ ë³€ìˆ˜ë“¤ì„ ì œê±°í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
    
    def _identify_column_types(self):
        """ë²”ì£¼í˜•ê³¼ ì—°ì†í˜• ë³€ìˆ˜ ìë™ êµ¬ë¶„ (train set ê¸°ì¤€)"""
        self.categorical_cols = self.X_train.select_dtypes(include=['object']).columns.tolist()
        self.numeric_cols = self.X_train.select_dtypes(include=[np.number]).columns.tolist()
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
        if not self.categorical_cols:
            self.categorical_cols = []
    
    def _build_preprocessor(self):
        """ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±"""
        transformers = []
        
        # ì—°ì†í˜• ë³€ìˆ˜: StandardScaler
        if self.numeric_cols:
            transformers.append(('num', StandardScaler(), self.numeric_cols))
        
        # ë²”ì£¼í˜• ë³€ìˆ˜: OneHotEncoder
        if self.categorical_cols:
            transformers.append(('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), 
                                self.categorical_cols))
        
        if transformers:
            self.preprocessor = ColumnTransformer(
                transformers=transformers,
                remainder='passthrough'
            )
        else:
            # ë³€ìˆ˜ê°€ ì—†ëŠ” ê²½ìš° (ì´ìƒí•œ ê²½ìš°)
            self.preprocessor = StandardScaler()
        
    def print_cls_results(self, y_test, y_pred, model_name):
        """Binary classification ê²°ê³¼ ì¶œë ¥"""
        precision = precision_score(y_test, y_pred, average='binary', zero_division=0)
        recall = recall_score(y_test, y_pred, average='binary', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
        acc = accuracy_score(y_test, y_pred)
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        
        df = pd.DataFrame({
            "Y_True": y_test,
            "Y_Pred": y_pred
        })
        
        print(f"\nğŸ§© Binary Classification ({model_name})")
        print(df.head(10).to_string(index=False))
        print(f"\nğŸ“Š Metrics:")
        print(f"  Accuracy:  {acc:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall:    {recall:.4f}")
        print(f"  F1-Score:  {f1:.4f}")
        print(f"\nğŸ“‹ Confusion Matrix:")
        print(f"              Predicted")
        print(f"              0     1")
        print(f"  Actual 0  {cm[0,0]:4d}  {cm[0,1]:4d}")
        print(f"         1  {cm[1,0]:4d}  {cm[1,1]:4d}")
        print("=" * 60)

    def get_models(self):
        """Binary classification ëª¨ë¸ë“¤ ë°˜í™˜"""
        models = {
            # ê¸°ë³¸ ëª¨ë¸ë“¤
            "LogisticRegression": LogisticRegression(
                max_iter=self.epoch, 
                random_state=self.random_state,
                class_weight='balanced'
            ),
            "SVC": SVC(
                kernel="rbf", 
                C=1.0, 
                probability=True, 
                random_state=self.random_state,
                class_weight='balanced'
            ),
            "RandomForest": RandomForestClassifier(
                n_estimators=self.n_estimators, 
                random_state=self.random_state, 
                n_jobs=self.n_jobs,
                class_weight='balanced'
            ),
            "GradientBoosting": GradientBoostingClassifier(
                random_state=self.random_state,
                n_estimators=self.n_estimators,
                learning_rate=self.lr
            ),
            "MLP": MLPClassifier(
                hidden_layer_sizes=(256, 128), 
                max_iter=self.epoch, 
                random_state=self.random_state
            ),
            
            # ì¶”ê°€ ëª¨ë¸ë“¤
            "AdaBoost": AdaBoostClassifier(
                n_estimators=self.n_estimators,
                learning_rate=self.lr,
                random_state=self.random_state
            ),
            "ExtraTrees": ExtraTreesClassifier(
                n_estimators=self.n_estimators,
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                class_weight='balanced'
            ),
            "DecisionTree": DecisionTreeClassifier(
                random_state=self.random_state,
                class_weight='balanced'
            ),
            "KNN": KNeighborsClassifier(
                n_neighbors=5,
                weights='distance'
            ),
            "NaiveBayes": GaussianNB()
        }
        
        # XGBoost (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€)
        if XGBOOST_AVAILABLE:
            models["XGBoost"] = xgb.XGBClassifier(
                n_estimators=self.n_estimators,
                learning_rate=self.lr,
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                eval_metric='logloss',
                use_label_encoder=False
            )
        
        # LightGBM (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€)
        if LIGHTGBM_AVAILABLE:
            models["LightGBM"] = lgb.LGBMClassifier(
                n_estimators=self.n_estimators,
                learning_rate=self.lr,
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                class_weight='balanced',
                verbose=-1
            )
        
        # CatBoost (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€)
        if CATBOOST_AVAILABLE:
            models["CatBoost"] = CatBoostClassifier(
                iterations=self.n_estimators,
                learning_rate=self.lr,
                random_state=self.random_state,
                verbose=False,
                thread_count=self.n_jobs
            )
        
        return models
    
    def list_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        models = self.get_models()
        print("=" * 60)
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡")
        print("=" * 60)
        print(f"ì´ {len(models)}ê°œ ëª¨ë¸:")
        for i, name in enumerate(models.keys(), 1):
            print(f"  {i:2d}. {name}")
        
        # Optional ëª¨ë¸ ìƒíƒœ
        print("\nğŸ“¦ Optional ëª¨ë¸ ìƒíƒœ:")
        print(f"  - XGBoost: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if XGBOOST_AVAILABLE else 'âŒ ì„¤ì¹˜ í•„ìš” (pip install xgboost)'}")
        print(f"  - LightGBM: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if LIGHTGBM_AVAILABLE else 'âŒ ì„¤ì¹˜ í•„ìš” (pip install lightgbm)'}")
        print(f"  - CatBoost: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if CATBOOST_AVAILABLE else 'âŒ ì„¤ì¹˜ í•„ìš” (pip install catboost)'}")
        print("=" * 60)
        return list(models.keys())

    def run_all(self, printf=True):
        """ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ - xë¥¼ ë„£ìœ¼ë©´ training í›„ ê²°ê³¼ ë°˜í™˜"""
        models = self.get_models()
        results = []
        self.trained_models = {}  # í•™ìŠµëœ ëª¨ë¸ ì €ì¥

        for name, clf in tqdm(models.items(), desc="Classifierë³„ ì§„í–‰", ncols=80,
                              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'):
            if printf:
                print(f"\nğŸš€ {name} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
                print("-" * 60)

            # ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ í•™ìŠµ
            clf.fit(self.X_train_processed, self.y_train)
            y_pred = np.array(clf.predict(self.X_test_processed)).ravel()

            # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
            self.trained_models[name] = clf

            # Binary classification metrics
            acc = accuracy_score(self.y_test, y_pred)
            precision = precision_score(self.y_test, y_pred, average='binary', zero_division=0)
            recall = recall_score(self.y_test, y_pred, average='binary', zero_division=0)
            f1 = f1_score(self.y_test, y_pred, average='binary', zero_division=0)

            cls_df = pd.DataFrame({
                "y_true": self.y_test,
                "y_pred": y_pred
            })

            if printf:
                self.print_cls_results(self.y_test, y_pred, name)

            results.append({
                "Model": name,
                "Accuracy": round(acc, 4),
                "Precision": round(precision, 4),
                "Recall": round(recall, 4),
                "F1": round(f1, 4),
                "cls_df": cls_df,
                "model": clf  # ëª¨ë¸ ê°ì²´ë„ ì €ì¥
            })

        df_result = pd.DataFrame(results).sort_values("F1", ascending=False)
        if printf:
            print("\nğŸ“Š ì „ì²´ ë¶„ë¥˜ ëª¨ë¸ ë¹„êµ ê²°ê³¼ ìš”ì•½")
            print(df_result[["Model", "Accuracy", "Precision", "Recall", "F1"]].to_string(index=False))
        return df_result
    
    def get_model(self, model_name):
        """í•™ìŠµëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (run_all() ì‹¤í–‰ í›„ ì‚¬ìš© ê°€ëŠ¥)"""
        if not hasattr(self, 'trained_models') or not self.trained_models:
            raise ValueError("ë¨¼ì € run_all()ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        
        if model_name not in self.trained_models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.trained_models.keys())}")
        
        return self.trained_models[model_name]
    
    def predict_proba(self, X_new, model_name=None):
        """ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥  (run_all() ì‹¤í–‰ í›„ ì‚¬ìš© ê°€ëŠ¥)"""
        if not hasattr(self, 'trained_models') or not self.trained_models:
            raise ValueError("ë¨¼ì € run_all()ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        
        if model_name is None:
            raise ValueError("model_nameì„ ì§€ì •í•´ì£¼ì„¸ìš”. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: " + ", ".join(self.trained_models.keys()))
        
        if model_name not in self.trained_models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.trained_models.keys())}")
        
        # X_newê°€ DataFrameì´ ì•„ë‹ˆë©´ ë³€í™˜
        if not isinstance(X_new, pd.DataFrame):
            X_new = pd.DataFrame(X_new, columns=self.feature_names)
        
        # ì „ì²˜ë¦¬ ì ìš©
        X_new_processed = self.preprocessor.transform(X_new)
        
        # í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
        model = self.trained_models[model_name]
        y_pred_proba = model.predict_proba(X_new_processed)[:, 1]  # í´ë˜ìŠ¤ 1ì˜ í™•ë¥ 
        
        return y_pred_proba
    
    def predict(self, X_new, model_name=None):
        """ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ (run_all() ì‹¤í–‰ í›„ ì‚¬ìš© ê°€ëŠ¥)"""
        if not hasattr(self, 'trained_models') or not self.trained_models:
            raise ValueError("ë¨¼ì € run_all()ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        
        if model_name is None:
            raise ValueError("model_nameì„ ì§€ì •í•´ì£¼ì„¸ìš”. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: " + ", ".join(self.trained_models.keys()))
        
        if model_name not in self.trained_models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.trained_models.keys())}")
        
        # X_newê°€ DataFrameì´ ì•„ë‹ˆë©´ ë³€í™˜
        if not isinstance(X_new, pd.DataFrame):
            X_new = pd.DataFrame(X_new, columns=self.feature_names)
        
        # ì „ì²˜ë¦¬ ì ìš©
        X_new_processed = self.preprocessor.transform(X_new)
        
        # í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡
        model = self.trained_models[model_name]
        y_pred = model.predict(X_new_processed)
        y_pred_proba = model.predict_proba(X_new_processed)[:, 1]  # í´ë˜ìŠ¤ 1ì˜ í™•ë¥ 
        
        return y_pred, y_pred_proba


class SoftVotingClassifierTrainer(ClassifierTrainer):
    """
    Soft Voting Classifier Trainer
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ í‰ê· ë‚´ì–´ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰
    """
    def __init__(self, X, y, X_test=None, y_test=None, random_state=42, n_estimators=500, epoch=100, lr=0.05, n_jobs=-1):
        """ClassifierTrainerì™€ ë™ì¼í•œ ì´ˆê¸°í™”"""
        super().__init__(X, y, X_test, y_test, random_state, n_estimators, epoch, lr, n_jobs)
        self.voting_models = {}  # Votingì— ì‚¬ìš©í•  ëª¨ë¸ë“¤ ì €ì¥
    
    def run_all(self, printf=True):
        """ëª¨ë“  ëª¨ë¸ í•™ìŠµ í›„ Soft Votingìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡"""
        models = self.get_models()
        self.trained_models = {}  # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        self.voting_models = {}  # Votingì— ì‚¬ìš©í•  ëª¨ë¸ë“¤ (predict_proba ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ)
        
        # 1ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ í•™ìŠµ
        if printf:
            print("=" * 60)
            print("1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ í•™ìŠµ")
            print("=" * 60)
        
        for name, clf in tqdm(models.items(), desc="ê°œë³„ ëª¨ë¸ í•™ìŠµ", ncols=80,
                              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'):
            try:
                # ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ í•™ìŠµ
                clf.fit(self.X_train_processed, self.y_train)
                
                # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
                self.trained_models[name] = clf
                
                # predict_probaë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ votingì— í¬í•¨
                if hasattr(clf, 'predict_proba'):
                    self.voting_models[name] = clf
                    
            except Exception as e:
                if printf:
                    print(f"âš ï¸  {name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
                continue
        
        if printf:
            print(f"\nâœ… ì´ {len(self.trained_models)}ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            print(f"âœ… Soft Votingì— {len(self.voting_models)}ê°œ ëª¨ë¸ ì‚¬ìš©")
            print("=" * 60)
        
        # 2ë‹¨ê³„: Soft Votingìœ¼ë¡œ ì˜ˆì¸¡
        if printf:
            print("\n2ë‹¨ê³„: Soft Voting ì˜ˆì¸¡")
            print("=" * 60)
        
        # ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥  í‰ê·  ê³„ì‚°
        y_proba_sum = np.zeros(len(self.y_test))
        
        for name, clf in self.voting_models.items():
            try:
                y_proba = clf.predict_proba(self.X_test_processed)[:, 1]  # í´ë˜ìŠ¤ 1ì˜ í™•ë¥ 
                y_proba_sum += y_proba
            except Exception as e:
                if printf:
                    print(f"âš ï¸  {name} ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        # í‰ê·  í™•ë¥  ê³„ì‚°
        y_proba_avg = y_proba_sum / len(self.voting_models)
        
        # ì„ê³„ê°’ 0.5ë¡œ ìµœì¢… ì˜ˆì¸¡
        y_pred = (y_proba_avg >= 0.5).astype(int)
        
        # 3ë‹¨ê³„: í‰ê°€
        acc = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred, average='binary', zero_division=0)
        recall = recall_score(self.y_test, y_pred, average='binary', zero_division=0)
        f1 = f1_score(self.y_test, y_pred, average='binary', zero_division=0)
        
        cls_df = pd.DataFrame({
            "y_true": self.y_test,
            "y_pred": y_pred
        })
        
        if printf:
            print(f"\nğŸ“Š Soft Voting ê²°ê³¼:")
            print(f"  Accuracy:  {acc:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall:    {recall:.4f}")
            print(f"  F1-Score:  {f1:.4f}")
            print(f"\nğŸ“‹ Confusion Matrix:")
            cm = confusion_matrix(self.y_test, y_pred)
            print(f"              Predicted")
            print(f"              0     1")
            print(f"  Actual 0  {cm[0,0]:4d}  {cm[0,1]:4d}")
            print(f"         1  {cm[1,0]:4d}  {cm[1,1]:4d}")
            print("=" * 60)
        
        # ê²°ê³¼ë¥¼ DataFrame í˜•íƒœë¡œ ë°˜í™˜ (ê¸°ì¡´ í˜•ì‹ê³¼ í˜¸í™˜)
        results = [{
            "Model": "SoftVoting",
            "Accuracy": round(acc, 4),
            "Precision": round(precision, 4),
            "Recall": round(recall, 4),
            "F1": round(f1, 4),
            "cls_df": cls_df,
            "model": None  # Votingì€ ë‹¨ì¼ ëª¨ë¸ ê°ì²´ê°€ ì•„ë‹˜
        }]
        
        # ê°œë³„ ëª¨ë¸ ê²°ê³¼ë„ í¬í•¨ (ì„ íƒì‚¬í•­)
        if printf:
            print("\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
            individual_results = []
            for name, clf in self.trained_models.items():
                try:
                    y_pred_ind = clf.predict(self.X_test_processed)
                    acc_ind = accuracy_score(self.y_test, y_pred_ind)
                    precision_ind = precision_score(self.y_test, y_pred_ind, average='binary', zero_division=0)
                    recall_ind = recall_score(self.y_test, y_pred_ind, average='binary', zero_division=0)
                    f1_ind = f1_score(self.y_test, y_pred_ind, average='binary', zero_division=0)
                    
                    individual_results.append({
                        "Model": name,
                        "Accuracy": round(acc_ind, 4),
                        "Precision": round(precision_ind, 4),
                        "Recall": round(recall_ind, 4),
                        "F1": round(f1_ind, 4)
                    })
                except:
                    continue
            
            if individual_results:
                df_ind = pd.DataFrame(individual_results).sort_values("F1", ascending=False)
                print(df_ind[["Model", "Accuracy", "Precision", "Recall", "F1"]].to_string(index=False))
        
        df_result = pd.DataFrame(results)
        return df_result
    
    def predict_proba(self, X_new, model_name=None):
        """Soft Votingìœ¼ë¡œ ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°"""
        if not hasattr(self, 'voting_models') or not self.voting_models:
            raise ValueError("ë¨¼ì € run_all()ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
        
        # model_name íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì‹œ (Soft Votingì€ ëª¨ë“  ëª¨ë¸ ì‚¬ìš©)
        if not isinstance(X_new, pd.DataFrame):
            X_new = pd.DataFrame(X_new, columns=self.feature_names)
        
        # ì „ì²˜ë¦¬ ì ìš©
        X_new_processed = self.preprocessor.transform(X_new)
        
        # ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥  í‰ê·  ê³„ì‚°
        y_proba_sum = np.zeros(len(X_new))
        
        for name, clf in self.voting_models.items():
            try:
                y_proba = clf.predict_proba(X_new_processed)[:, 1]
                y_proba_sum += y_proba
            except Exception as e:
                continue
        
        # í‰ê·  í™•ë¥  ë°˜í™˜
        y_proba_avg = y_proba_sum / len(self.voting_models)
        return y_proba_avg
    
    def predict(self, X_new, model_name=None, threshold=0.5):
        """Soft Votingìœ¼ë¡œ ì˜ˆì¸¡"""
        y_proba = self.predict_proba(X_new, model_name)
        y_pred = (y_proba >= threshold).astype(int)
        return y_pred, y_proba

